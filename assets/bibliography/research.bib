@article{zzhiqiang2023,
  author = {Zuo, Zhiqiang and Niu, Xintao and Zhang, Siyi and Fang, Lu and Khoo, Siau Cheng and Lu, Shan and Sun, Chengnian and Xu, Guoqing Harry},
  title = {Toward More Efficient Statistical Debugging with Abstraction Refinement},
  year = {2023},
  issue_date = {March 2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {32},
  number = {2},
  issn = {1049-331X},
  html = {https://doi.org/10.1145/3544790},
  doi = {10.1145/3544790},
  abstract = {Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability.},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  month = {mar},
  articleno = {36},
  numpages = {38},
  keywords = {abstraction refinement, Statistical debugging, fault localization, selective instrumentation}, 
  selected={false},
  bibtex_show={true},
}

@article{wang2022adaptive,
  title={An Adaptive Penalty based Parallel Tabu Search for Constrained Covering Array Generation},
  author={Yan Wang and Huayao Wu and Xintao Niu and Changhai Nie and Jiaxi Xu},
  journal={Information and Software Technology},
  abbr={IST},
  volume={143},
  pages={106768},
  year={2022},
  publisher={Elsevier},
  bibtex_show={true},
  html = {https://doi.org/10.1016/j.infsof.2021.106768},
  doi = {10.1016/j.infsof.2021.106768}, 
}

@ARTICLE{niu2020interleaving,
  author={Xintao Niu and Changhai Nie and Hareton Leung and Yu Lei  and Xiaoyin Wang and Jiaxi Xu and Yan Wang},
  abbr={TSE},
  journal={IEEE Transactions on Software Engineering},
  title={An Interleaving Approach to Combinatorial Testing and Failure-Inducing Interaction Identification},
  year={2020},
  volume={46},
  number={6},
  pages={584-615},
  selected={true},
  bibtex_show={true},
  doi={10.1109/TSE.2018.2865772},
  html={https://ieeexplore.ieee.org/document/8438906},
  abstract={Combinatorial testing (CT) seeks to detect potential faults caused by various interactions of factors that can influence the software systems. When applying CT, it is a common practice to first generate a set of test cases to cover each possible interaction and then to identify the failure-inducing interaction after a failure is detected. Although this conventional procedure is simple and forthright, we conjecture that it is not the ideal choice in practice. This is because 1) testers desire to identify the root cause of failures before all the needed test cases are generated and executed 2) the early identified failure-inducing interactions can guide the remaining test case generation so that many unnecessary and invalid test cases can be avoided. For these reasons, we propose a novel CT framework that allows both generation and identification process to interact with each other. As a result, both generation and identification stages will be done more effectively and efficiently. We conducted a series of empirical studies on several open-source software, the results of which show that our framework can identify the failure-inducing interactions more quickly than traditional approaches while requiring fewer test cases.},
  }

@inproceedings{pei2017deepxplore,
author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
title = {DeepXplore: Automated Whitebox Testing of Deep Learning Systems},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132785},
doi = {10.1145/3132747.3132785},
abstract = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.We design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3\%.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {1–18},
numpages = {18},
keywords = {differential testing, whitebox testing, Deep learning testing},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{manuel2022intramorphic,
    author = {Rigger, Manuel and Su, Zhendong},
    title = {Intramorphic Testing: A New Approach to the Test Oracle Problem},
    year = {2022},
    isbn = {9781450399098},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    html = {https://doi.org/10.1145/3563835.3567662},
    doi = {10.1145/3563835.3567662},
    abstract = {A test oracle determines whether a system behaves correctly for a given input. Automatic testing techniques rely on an automated test oracle to test the system without user interaction. Important families of automated test oracles include Differential Testing and Metamorphic Testing, which are both black-box approaches; that is, they provide a test oracle that is oblivious to the system’s internals. In this work, we propose Intramorphic Testing as a white-box methodology to tackle the test oracle problem. To realize an Intramorphic Testing approach, a modified version of the system is created, for which, given a single input, a test oracle can be provided that relates the output of the original and modified systems. As a concrete example, by replacing a greater-equals operator in the implementation of a sorting algorithm with smaller-equals, it would be expected that the output of the modified implementation is the reverse output of the original implementation. In this paper, we introduce the methodology and illustrate it via a set of use cases.},
    booktitle = {Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
    pages = {128–136},
    numpages = {9},
    keywords = {test oracle problem, automated testing, white-box testing},
    location = {Auckland, New Zealand},
    series = {Onward! 2022}
}

@article{niu2021theory,
  title={A theory of pending schemas in combinatorial testing},
  author={Xintao Niu and Huayao Wu and Changhai Nie and Yu  Lei and Xiaoyin Wang},
  journal={IEEE Transactions on Software Engineering},
  abbr={TSE},
  year={2022},
  publisher={IEEE},
  volume={48},
  number={10},
  pages={4119--4151},
  selected={true},
  bibtex_show={true},
  html={https://ieeexplore.ieee.org/abstract/document/9543605},
  doi={10.1109/TSE.2021.3113920},
  abstract={Combinatorial Testing (CT) is an effective testing technique for detecting failures which are triggered by the interactions of various factors that influence the behaviour of a system. Although many studies in CT have designed elaborate test suites (called covering arrays) to systemically check each possible factor interaction, they provide weak support to locate the concrete failure-inducing interactions, i.e., the Minimal Failure-causing Schemas (MFS). To this end, a variety of MFS identification approaches have been proposed. However, as this study reveals, these approaches suffer from various issues such as cannot identify multiple overlapping MFSs, cannot handle MFSs with high degrees, cannot be applied to systems with large number of parameters, etc. These issues are essentially caused by the exponential computing complexity of checking every interaction in the test cases. Therefore, they can only focus on a subset of all the possible interactions, resulting in many interactions unnoticed. Ignoring these unnoticed interactions could potentially cause failures that have never been systematically checked. Hence, it is beneficial for MFS identification approaches to identify these interactions. In order to account for these unnoticed interactions in CT, this study introduces the notion of pending schema, based on which a theoretical framework of CT schemas is established. In particular, we formally define the determinability of a schema in CT with respect to given information; as such, the yet-to-be determined schemas are exactly the pending schemas. The relationships between the different schemas (faulty, healthy, and pending) and test cases are also theoretically analyzed. Based on which, we further propose three formulas, along with three corresponding algorithms, for the identification of the pending schemas in failing test cases, and formally prove their correctness. As a result, we reduce the complexity of obtaining pending schemas with respect to the number of factors that may have influences on the software.},
}

@article{niu2021enhance,
  title={Enhance Combinatorial Testing with Metamorphic Relations},
  abbr={TSE},
  author={Xintao Niu and Yanjie Sun and Huayao Wu and Gang Li and Changhai Nie and Yu  Lei and Xiaoyin Wang},
  journal={IEEE Transactions on Software Engineering},
  year={2022},
  publisher={IEEE},
  selected={true},
  volume={48},
  number={12},
  pages={5007--5029},
  bibtex_show={true},
  doi={10.1109/TSE.2021.3131548},
  html={https://ieeexplore.ieee.org/abstract/document/9629275},
  abstract={Due to the effectiveness and efficiency in detecting defects caused by interactions of multiple factors, Combinatorial Testing (CT) has received considerable scholarly attention in the last decades. Despite numerous practical test case generation techniques being developed, there remains a paucity of studies addressing the automated oracle generation problem, which holds back the overall automation of CT. As a consequence, much human intervention is inevitable, which is time-consuming and error-prone. This costly manual task also restricts the application of higher testing strength, inhibiting the full exploitation of CT in industrial practice. To bridge the gap between test designs and fully automated test flows, and to extend the applicability of CT, this paper presents a novel CT methodology, named COMER, to enhance the traditional CT by accounting for Metamorphic Relations (MRs). COMER puts a high priority on generating pairs of test cases which match the input rules of MRs, i.e., the Metamorphic Group (MG), such that the correctness can be automatically determined by verifying whether the outputs of these test cases violate their MRs. As a result, COMER can not only satisfy the t-way coverage as what CT does, but also automatically check as many test oracle violations as possible. Several empirical studies conducted on 31 real-world software projects have shown that COMER increased the number of metamorphic groups by an average factor of 75.9 and also increased the failure detection rate by an average factor of 11.3, when compared with CT, while the overall number of test cases generated by COMER barely increased.},
}

@ARTICLE{niu2020identifying,
  author={Xintao Niu and Changhai Nie and Yu Lei and Hareton Leung and Xiaoyin Wang},
  abbr={TSE},
  journal={IEEE Transactions on Software Engineering},
  title={Identifying Failure-Causing Schemas in the Presence of Multiple Faults},
  year={2020},
  volume={46},
  number={2},
  pages={141-162},
  keywords={Testing;Bars;Fault diagnosis;Computer bugs;Software algorithms;Open source software;Software testing;combinatorial testing;failure-causing schemas;masking effects},
  doi={10.1109/TSE.2018.2844259},
  html={https://ieeexplore.ieee.org/document/8372636},
  abstract={Combinatorial testing (CT) has been proven effective in revealing the failures caused by the interaction of factors that affect the behavior of a system. The theory of Minimal Failure-Causing Schema (MFS) has been proposed to isolate the cause of a failure after CT. Most algorithms that aim to identify MFS focus on handling a single fault in the System Under Test (SUT). However, we argue that multiple faults are more common in practice, under which masking effects may be triggered so that some failures cannot be observed. The traditional MFS theory lacks a mechanism to handle such effects; hence, they may incorrectly isolate the MFS. To address this problem, we propose a new MFS model that takes into account multiple faults. We first formally analyze the impact of the multiple faults on existing MFS identifying algorithms, especially in situations where masking effects are triggered by multiple faults. We then develop an approach that can assist traditional algorithms to better handle multiple faults. Empirical studies were conducted using several kinds of open-source software, which showed that multiple faults with masking effects do negatively affect traditional MFS identifying approaches and that our approach can help to alleviate these effects.},
  ISSN={2326-3881},
  month={Feb},
  selected={true},
  bibtex_show={true},
}